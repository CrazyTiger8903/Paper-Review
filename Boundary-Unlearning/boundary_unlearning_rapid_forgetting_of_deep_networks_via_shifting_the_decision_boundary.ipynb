{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOf8TY2TZA0cpkWY/2zudXg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CrazyTiger8903/Paper-Review/blob/main/Boundary-Unlearning/boundary_unlearning_rapid_forgetting_of_deep_networks_via_shifting_the_decision_boundary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# boundary unlearning: rapid forgetting of deep networks via shifting the decision boundary(CVPR 2023)"
      ],
      "metadata": {
        "id": "QHmQ3CsHFzKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Introduction\n",
        "\n",
        "---\n",
        "\n",
        "- Machine Unlearing : ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì¼ë¶€ í•™ìŠµ ë°ì´í„°ë¥¼ ìŠë„ë¡ í•˜ê±°ë‚˜ ê·¸ ê³„í†µì„ ì‚­ì œì‹œí‚¤ëŠ” ê²ƒì„ ì˜ë¯¸\n",
        "\n",
        "- ì™œ í•„ìš”í•œê°€?\n",
        "  1.   \"the right to be forgotten\" : ê°œì¸ ë°ì´í„°ì˜ ì‚­ì œë¥¼ ìš”ì²­í•˜ë©´ ê¸°ì—…ì€ ë°˜ë“œì‹œ ì‚­ì œí•´ì•¼í•œë‹¤(GDPR)\n",
        "  2.   ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ìœ ìš©\n",
        "    -   ë°ì´í„° ì¤‘ë… ê³µê²©(í•™ìŠµ ë°ì´í„°ë¥¼ ì¡°ì‘í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ ì €í•˜ ë˜ëŠ” ì˜ëª» íŒë‹¨í•˜ë„ë¡ í•˜ëŠ” ê³µê²©)\n",
        "    -   ì‹œê°„ì´ ì§€ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ í•™ìŠµ ë°ì´í„°\n",
        "    -   í•™ìŠµ í›„ í•™ìŠµ ë°ì´í„°ê°€ ì˜¤ë¥˜ë¡œ íŒë‹¨\n",
        "\n",
        "- ì´ˆê¸°ì—°êµ¬\n",
        "  1.   ì²˜ìŒ ë¶€í„° retraining -> ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼\n",
        "  2.   ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ scrubbing(ì •ë¦¬)í•˜ì—¬ forgetting data(ì‚­ì œ ëŒ€ìƒ ë°ì´í„°)ì˜ ì˜í–¥ì„ ì œê±° -> íŒŒë¼ë¯¸í„° space ì°¨ì›ì´ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì— costê°€ ë¹„ì‹¸ë‹¤.\n",
        "\n",
        "- Retrained Modelì˜ ê²°ì • ê³µê°„ íŠ¹ì§•\n",
        "  1.   forgetting samplesê°€ ëª¨ë¸ì˜ ê²°ì • ê³µê°„ ì „ì²´ì— í¼ì ¸ ìˆë‹¤. ì¦‰ forgetting samplesì˜ ê²°ì • ê²½ê³„ê°€ ê¹¨ì¡Œë‹¤.\n",
        "  2.   ëŒ€ë¶€ë¶„ì˜ forgetting samplesëŠ” ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì˜ ê²½ê³„ë¡œ ì´ë™\n",
        "    - closest-to-boundary criterion : ê²°ì • ê³µê°„ì˜ í´ëŸ¬ìŠ¤í„° ê²½ê³„ì— ìˆëŠ” ìƒ˜í”Œì€ ì˜ˆì¸¡ ì‹œ ë†’ì€ ë¶ˆí™•ì‹¤ì„±ì„ ë³´ì¼ ê°€ëŠ¥ì„±ì´ í¬ë‹¤.\n",
        "  3. ì´ëŸ¬í•œ íŠ¹ì§•ì€ machine unlearningì˜ 2ê°€ì§€ ëª©í‘œì— ìì—°ìŠ¤ëŸ½ê²Œ ë¶€í•©í•œë‹¤.\n",
        "    - Utility guarantee(ìœ ìš©ì„± ë³´ì¥) : forgetting data(ì‚­ì œ ëŒ€ìƒ ë°ì´í„°)ì— ëŒ€í•´ì„œëŠ” ì¼ë°˜í™” ì„±ëŠ¥ì´ ë‚®ì•„ì•¼ í•˜ë©°, ë‚˜ë¨¸ì§€ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì„±ëŠ¥ì€ ìœ ì§€ë˜ì–´ì•¼ í•œë‹¤.\n",
        "    - privacy guarantee : unlearned ëª¨ë¸ì€ ì‚­ì œ ëŒ€ìƒ ë°ì´í„°ì˜ ì •ë³´ë¥¼ ìœ ì¶œí•´ì„œëŠ” ì•ˆëœë‹¤.\n",
        "    - ì´ ë…¼ë¬¸ì—ì„œëŠ” forgetting dataì˜ ê²½ê³„ë§Œ íŒŒê´´í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ìœ ì§€í•¨ìœ¼ë¡œì¨ Utility guaranteeë¥¼ ë‹¬ì„±í•˜ê³ , forgetting dataë¥¼ ë‹¤ë¥¸ í´ëŸ¬ìŠ¤í„°ì˜ ê²½ê³„ë¡œ ë°€ì–´ëƒ„ìœ¼ë¡œì¨ privacy guaranteeë¥¼ ë‹¬ì„±í•œë‹¤.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1KuoVnQL0p0YejkocQIq172TBEVomaasD'/>\n",
        "\n",
        "- Boundary Unlearning\n",
        "  1. ê²°ì • ê³µê°„ì—ì„œ ê²°ì • ê²½ê³„ë¥¼ ì´ë™ì‹œì¼œ ë¹ ë¥´ê³  íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ì—ì„œ ì‚­ì œ ëŒ€ìƒ í´ë˜ìŠ¤ë¥¼ ìŠê²Œ í•˜ëŠ” ë°©ë²•ì¸ \"Boundary Unlearning\" ì œì•ˆ\n",
        "  2. Boundary Shrink : forgetting class(ì‚­ì œ ëŒ€ìƒ í´ë˜ìŠ¤)ë¥¼ ë‹¤ë¥¸ í´ë˜ìŠ¤ë¡œ ë¶„í• í•˜ì—¬ ê²°ì • ê²½ê³„ë¥¼ íŒŒê´´.\n",
        "  3. Boundary Expanding : forgetting ë°ì´í„°ë¥¼ shadow classë¡œ ë§¤í•‘ í›„ ì´ í´ë˜ìŠ¤ë¥¼ ì œê±°í•˜ì—¬ í™œì„±í™”ë¥¼ ë¶„ì‚°.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hQdo0cLZnbF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Related Work\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wVRwqnKGxGbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preliminaries and Notation\n",
        "---\n",
        "1. $D$ : training dataset. ì „ì²´ í•™ìŠµ ë°ì´í„°ì…‹ìœ¼ë¡œ, ì…ë ¥ë°ì´í„° xì™€ í´ë˜ìŠ¤ ë ˆì´ë¸” yë¡œ êµ¬ì„±\n",
        "\n",
        "2. $D_{f} $ : Forgetting Data. ì‚­ì œí•´ì•¼ í•  ë°ì´í„°ì˜ ë¶€ë¶„ ì§‘í•©. ì£¼ë¡œ íŠ¹ì • í´ë˜ìŠ¤ì˜ ëª¨ë“  ìƒ˜í”Œë¡œ êµ¬ì„±ë¨. (ì˜ˆë¥¼ ë“¤ë©´, dog í´ë˜ìŠ¤ ì „ì²´ë¥¼ ì‚­ì œ.)\n",
        "\n",
        "3. $D_{r} $ :  Remaining Training Data. $D_{f} $(ì‚­ì œ ëŒ€ìƒ ë°ì´í„°)ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë°ì´í„°. ì¦‰ ìœ ì§€í•˜ë ¤ëŠ” ì •ë³´ë“¤\n",
        "\n",
        "4. $f_{ w_{0} } $ : í•™ìŠµë°ì´í„° Dì— ëŒ€í•´ í•™ìŠµëœ Original ëª¨ë¸. w0ë¡œ íŒŒë¼ë¯¸í„°í™” ë˜ì–´ ìˆë‹¤. $f_{ w_{0} }(x) $ëŠ” í•™ìŠµëœ ëª¨ë¸ì´ xì— ëŒ€í•´ ì¶œë ¥í•˜ëŠ” ë¡œì§“ì„ ë‚˜íƒ€ë‚¸ë‹¤.\n",
        "\n",
        "5. $f_{w*} $ : retrained ëª¨ë¸($D_{r} $ë¡œ ë‹¤ì‹œ í•™ìŠµëœ ëª¨ë¸) ìµœì ì˜ ë°ì´í„° ì‚­ì œ ëª¨ë¸ë¡œ ê°„ì£¼.\n",
        "\n",
        "6. $f_{w'}$ : ë°ì´í„° ì‚­ì œë¥¼ ìˆ˜í–‰í•œ í›„ ì–»ì–´ì§„ ëª¨ë¸.\n",
        "\n",
        "- ìµœì¢… ëª©í‘œ : fw'(ë°ì´í„° ì‚­ì œ í›„ ì–»ì–´ì§„ ëª¨ë¸)ì´ fw*(ì¬í•™ìŠµ ì‹œí‚¨ ëª¨ë¸)ì™€ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ ë§Œë“œëŠ” ê²ƒ.\n",
        "\n",
        "7. decision boundary(ê²°ì •ê²½ê³„) : ì…ë ¥ xê°€ í´ë˜ìŠ¤ iì™€ j ì¤‘ ì–´ëŠ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë ì§€ ê²°ì •í•˜ëŠ” ê²½ê³„. ë‘ í´ë˜ìŠ¤ì˜ í™•ë¥ ì´ ë™ì¼í•´ì§€ëŠ” ì§€ì "
      ],
      "metadata": {
        "id": "YmxaVZBLxOe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Proposed Methods\n",
        "---\n",
        "### 4.1. Boundary Shrink\n",
        "- Random Labeling\n",
        "  - ëœë¤í•˜ê²Œ ë¼ë²¨ë§ëœ forgetting ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ íŒŒì¸íŠœë‹ì„ ì§„í–‰í•˜ëŠ” ë°©ë²•\n",
        "  - ê°€ì¥ ì§ê´€ì ì¸ ë°©ë²•ì´ì§€ë§Œ, ë‚˜ë¨¸ì§€ í´ë˜ìŠ¤ì˜ ê²½ê³„ë„ ëœë¤í•˜ê²Œ ì´ë™ ì‹œì¼œ ë‚˜ë¨¸ì§€ ë°ì´í„°ì— ëŒ€í•œ ëª¨ë¸ utilityì €í•˜\n",
        "\n",
        "- Retrained ëª¨ë¸ì˜ ê²°ì • ê²½ê³„ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯, ëŒ€ë¶€ë¶„ì˜ forgetting ìƒ˜í”Œì€ ëœë¤í•œ í´ë˜ìŠ¤ê°€ ì•„ë‹Œ íŠ¹ì • í´ë˜ìŠ¤ë“¤ë¡œ ì˜ˆì¸¡. ì¦‰, íŠ¹ì§• ê³µê°„ì—ì„œ forgetting ìƒ˜í”Œê³¼ ë‹¤ë¥¸ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œë“¤ ê°„ì˜ ìœ ì‚¬ì„± ë°œê²¬ì´ ì¤‘ìš”í•˜ë‹¤.\n",
        "\n",
        "- ì œì•ˆ\n",
        "  - ì ëŒ€ì  ê³µê²©(adversarial attacks)ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ì›ƒ íƒìƒ‰ ë°©ë²•ì„ ì œì•ˆ\n",
        "    - ê°€ì¥ ê°€ê¹Œìš´ ê²°ì • ê²½ê³„ë¥¼ ë„˜ì–´ ì ëŒ€ì  ì˜ˆì œ(adversarial examples)ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë°©ë²•\n",
        "    - forgetting ìƒ˜í”Œì˜ ê²°ì • ê²½ê³„ë¥¼ ì´ë™ì‹œí‚¤ëŠ” ë°©í–¥ì„ ì•Œë ¤ì£¼ë©°, ì›ë˜ ëª¨ë¸ì—ì„œ cross samples(ì˜ëª»ëœ ì˜ˆì¸¡)ë¥¼ ì˜ˆì¸¡í•¨ìœ¼ë¡œì¨ ê°€ì¥ ê°€ê¹Œìš°ë©´ì„œë„ ì˜ëª»ëœ ë¼ë²¨ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n",
        "  - cross samplesì˜ ë¼ë²¨ì„ í•´ë‹¹í•˜ëŠ” forgetting ìƒ˜í”Œì— í• ë‹¹\n",
        "  - ì¬í• ë‹¹ ëœ ëª¨ë“  ìƒ˜í”Œë¡œ ì›ë˜ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë©´, forgetting í´ë˜ìŠ¤ì˜ ê²½ê³„ê°€ ì •í™•í•œ ë°©í–¥ìœ¼ë¡œ ì¶•ì†Œë  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "1. Original ëª¨ë¸ í•™ìŠµ\n",
        "  - $ w_0 = \\arg\\min_w \\sum_{(x_i, y_i) \\in D} L(x_i, y_i, w) $\n",
        "  - $w_0$(original ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜)\n",
        "  - D(ì „ì²´ ë°ì´í„°ì„¸íŠ¸)ì— ëŒ€í•´ ì†ì‹¤ Lì„ ìµœì†Œí™”í•˜ëŠ” ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë‚˜íƒ€ëƒ„\n",
        "2. ì´ì›ƒê²€ìƒ‰ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš°ë©´ì„œ ì˜ëª»ëœ ë ˆì´ë¸” ì°¾ê¸°\n",
        "  - $x'_f = x_f + \\epsilon \\cdot \\text{sign} \\left( \\nabla_{x_f} L(x_f, y, w_0) \\right)$\n",
        "  - original ëª¨ë¸ì˜ ì†ì‹¤ í•¨ìˆ˜ì˜ $ x_{f}$ì— ëŒ€í•œ gradient ê³„ì‚°\n",
        "  - signì„ í†µí•´ gradientì˜ ë¶€í˜¸ë¥¼ ì·¨í•´ ë°©í–¥ì„ ê²°ì •. ì¦‰ í¬ê¸°ëŠ” ì œì™¸í•˜ê³  ë°©í–¥ì„± ë§Œì„ ì¶”ì¶œí•œë‹¤.\n",
        "  - gradientì˜ ë°©í–¥ì„±ì— ì¼ì •í•œ í¬ê¸°ì˜ ë…¸ì´ì¦ˆ í¬ê¸° $\\epsilon$(ì—¡ì‹¤ë¡ )ì„ ì¶”ê°€\n",
        "  - ì´ë¥¼ í†µí•´ $ x_{f}$ì˜ ê²°ì • ê²½ê³„ë¥¼ ë„˜ë„ë¡ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•œë‹¤. - ì ëŒ€ì  ê³µê²©ê³¼ ìœ ì‚¬(PGD Attack)\n",
        "  - $y_{nbi} \\leftarrow \\text{softmax} \\left( f_{w_0}(x'_f) \\right)$\n",
        "  - $x'_f$(cross sample)ì— ëŒ€í•´ original ëª¨ë¸ $f_{w_0}$ë¥¼ ì‚¬ìš©í•´ ê°€ì¥ ê°€ê¹ì§€ë§Œ ì˜ëª»ëœ ë ˆì´ë¸” $y_{nbi}$ë¥¼ ì˜ˆì¸¡\n",
        "3. fine-tunning\n",
        "  - $w' = \\arg \\min_w \\sum_{(x_i, y_{nbi}) \\in D_f} L(x_i, y_{nbi}, w_0)$\n",
        "  - forgetting í´ë˜ìŠ¤ì˜ boundary shrinkë¥¼ ìœ„í•´ ì¬í• ë‹¹ëœ ìƒ˜í”Œë¡œ original ëª¨ë¸ì„ íŒŒì¸ íŠœë‹. ì´ë¥¼ í†µí•´ ìµœì ì˜ íŒŒë¼ë¯¸í„° w'ì„ íšë“\n",
        "\n",
        "- ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ í•™ìŠµí•œ ë°ì´í„° ê°„ì˜ ê´€ê³„ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ ê²°ì • ê²½ê³„ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆê²Œ ëœë‹¤.\n",
        "\n",
        "### 4.2. Boundary Expanding\n",
        "\n"
      ],
      "metadata": {
        "id": "pdp77eYVxSyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Evaluation\n",
        "\n",
        "---\n",
        "1. Utility Guarantee\n",
        "  - ëª¨ë¸ ì •í™•ë„ë¥¼ í†µí•´ í‰ê°€\n",
        "  - ë‹¤ë¥¸ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ê³¼ ë¹„êµí•˜ì˜€ì„ë•Œ, Boundary ShrinkëŠ” Dfì— ëŒ€í•œ ì •í™•ë„ë¥¼ í¬ê²Œ ì¤„ì´ë©´ì„œ, Drì— ëŒ€í•œ ì •í™•ë„ë¥¼ ìµœì†Œí•œìœ¼ë¡œ ê°ì†Œì‹œí‚¨ë‹¤.\n",
        "  - Boundary Expandingì€ Dfì— ëŒ€í•œ ì •í™•ë„ê°€ ì¡°ê¸ˆ ë” ë†’ì§€ë§Œ, ì„±ëŠ¥ê³¼ ì‹œê°„ ì‚¬ì´ì˜ ê· í˜•ì„ ë§ì¶˜ ë¹ ë¥¸ ëŒ€ì•ˆìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1VOQwfeJkygY4VMXLg5UvJ7YypsJ966Yc'/>\n",
        "\n",
        "2. Privacy Guarantee\n",
        "  - ASR(Attack Success Rate)ë¥¼ í†µí•´ í‰ê°€\n",
        "  - ASRì´ 100%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ Dfì— ëŒ€í•œ ì •ë³´ê°€ ëœ ì œê±°ë˜ì—ˆìŒì„ ì˜ë¯¸\n",
        "  - ASRì´ 0%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ Streisand íš¨ê³¼ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ(Dfì˜ ëª¨ë“  ìƒ˜í”Œì´ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡ë˜ì–´, ì‚­ì œ ë°ì´í„°ì— ëŒ€í•œ ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí•  ê°€ëŠ¥ì„±ì„ ì˜ë¯¸)\n",
        "  - Finetuneì˜ ê²½ìš° ë†’ì€ Utility ì„±ëŠ¥ì„ ë³´ì˜€ì§€ë§Œ, Privacy ì„±ëŠ¥ì˜ ê²½ìš° ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì„(ì‚­ì œ ë°ì´í„°ì— ëŒ€í•œ ì •ë³´ë¥¼ ê±°ì˜ ì œê±°í•˜ì§€ ëª»í–ˆìŒ)\n",
        "  - Boundary Shrinkì™€ Boundary Expanding ë°©ë²• ëª¨ë‘ ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„\n",
        "\n",
        "   <img src='https://drive.google.com/uc?id=18rlJDnCK9U9uAnrgLPMa3uH-JXBSaXjE'/>\n",
        "\n",
        "3. Computational Complexity\n",
        "  - Retrain ëŒ€ë¹„ ë§ì€ ì‹œê°„ ë‹¨ì¶•\n",
        "  - Boundary ShrinkëŠ” êµì°¨ ìƒ˜í”Œ ìƒì„±(cross-sample generation) ê³¼ì • ë•Œë¬¸ì— Boundary Expandingë³´ë‹¤ ì•½ê°„ ë” ë§ì€ ì‹œê°„ì´ ì†Œìš”\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1JCzMtLRl9GYfdGpXYOa5T-aSkCt0pRck'/>\n",
        "\n",
        "\n",
        "4. Attention Map\n",
        "  - Retrain ëª¨ë¸ì˜ attention mapì„ í†µí•´ unlearningë˜ì—ˆì„ ë•Œ ëª¨ë¸ì´ ì–¼êµ´ ì˜ì—­ì— ì§‘ì¤‘í•˜ì§€ ì•ŠëŠ” ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n",
        "  - Boundary Shrinkë¡œ unlearningëœ ê²½ìš° ëª¨ë¸ì˜ ì£¼ìœ„ë¥¼ ì˜¤ì§ ë°°ê²½ì—ë§Œ ì§‘ì¤‘ì‹œí‚¤ë„ë¡ í•œ ê²ƒì„ í™•ì¸\n",
        "  - Boundary Expandingì˜ ê²½ìš° ì£¼ìœ„ë¥¼ ì™„ì „íˆ ë°°ê²½ìœ¼ë¡œ ì „í™˜ì‹œí‚¤ì§€ ëª»í–ˆì§€ë§Œ, ì—¬ì „íˆ ì–¼êµ´ ì™¸ë¶€ ì˜ì—­ìœ¼ë¡œ ì§‘ì¤‘ì„ ì „í™˜ì‹œí‚¨ ê²ƒì„ í™•ì¸\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1C5Nz__kfF9uu9UrdQkiw9N0v6OarI9ng'/>\n",
        "\n",
        "5. Visualization of Decision Space\n",
        "  - Boundary Shrinkë¥¼ ì ìš©í•œ í›„ ê°€ì¥ ê°€ê¹Œìš´ í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡ë˜ëŠ” ê²ƒì„ í™•ì¸\n",
        "  - Retrain ëª¨ë¸ì²˜ëŸ¼ í´ëŸ¬ìŠ¤í„°ê°€ ì™„ì „íˆ í¼ì§€ì§€ëŠ” ì•Šì•˜ì§€ë§Œ, forgetting dataì˜ ê²°ì • ê³µê°„ì´ ê·¼ì²˜ì˜ í´ë˜ìŠ¤ì— ì˜í•´ ë‚˜ëˆ ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n",
        "  - Boundary Expandingì˜ ê²½ìš° forgetting dataì˜ í´ëŸ¬ìŠ¤í„°ê°€ ì¤‘ì‹¬ì—ì„œ ë°€ë ¤ë‚˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1M5hdFb0NjhRjBIjf21NGTrmitm3haGDe'/>\n",
        "\n",
        "6. Distribution of the Entropy of Model Output\n",
        "  - ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ìŒ : ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•  ë•Œ ë” í™•ì‹ ì„ ê°–ê³  ì˜ˆì¸¡í•¨ì„ ì˜ë¯¸\n",
        "  - Retrained Modelì€ Dfì˜ ì—”íŠ¸ë¡œí”¼ ìˆ˜ì¹˜ê°€ í¬ê²Œ ì¦ê°€í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n",
        "  - Boundary Shrinkì™€ Expandingì˜ ê²°ê³¼ ì—­ì‹œ Retrainedì˜ ê²°ê³¼ì™€ ìœ ì‚¬í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\n",
        "  - Random Labelsì˜ ê²½ìš° Dfì˜ ì—”íŠ¸ë¡œí”¼ ìˆ˜ì¹˜ê°€ ìƒë‹¹íˆ í° ê²ƒì„ ë³¼ ìˆ˜ ìˆëŠ”ë°, ì´ëŠ” Retrained modelê³¼ í° ì°¨ì´ë¥¼ ë³´ì¸ë‹¤.\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1VnyEitYD_dFq9ZcuTtdVGT00PB5aQ1U1'/>"
      ],
      "metadata": {
        "id": "81_JJpKWxTc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "---\n",
        "- ê²°ì • ê²½ê³„ë¥¼ ì´ë™ì‹œì¼œ í•™ìŠµëœ DNNì—ì„œ íŠ¹ì • í´ë˜ìŠ¤ ì „ì²´ì˜ ì •ë³´ë¥¼ ì œê±°í•˜ëŠ” ìµœì´ˆì˜ ë¨¸ì‹  ì–¸ëŸ¬ë‹ ê¸°ë²•ì¸ Boundary Unlearningì„ ì œì•ˆ\n",
        "- ê³¼ë„í•œ ê³„ì‚° ìì›ì„ ì†Œëª¨í•˜ì§€ ì•Šìœ¼ë©°, ê¸°ì¡´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì— ê°œì…í•˜ì§€ ì•ŠìŒ\n",
        "- Utilityì™€ Privacy ë³´ì¥ ëª¨ë‘ì—ì„œ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ë§ê° ì„±ëŠ¥ì„ ë³´ì„\n"
      ],
      "metadata": {
        "id": "ICcUkk3hwwuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì½”ë“œ êµ¬í˜„"
      ],
      "metadata": {
        "id": "v78q98tfzwqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Original & Retrain ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "1-1. pytorch ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ResNet18 ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  CIFAR-10 ë°ì´í„°ì…‹ì„ í•™ìŠµí•œ 'Original'ëª¨ë¸ì„ ë§Œë“¤ê³  ì •í™•ë„ ë“±ì„ í‰ê°€í•˜ì„¸ìš”.\n",
        "\n",
        "1-2. 10ê°œ í´ë˜ìŠ¤ ì¤‘ í•œ ê°œì˜ í´ë˜ìŠ¤ë¥¼ ì„ íƒí•´ ì´ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ 9ê°œ í´ë˜ìŠ¤ì˜ ë°ì´í„°ë¡œ í•™ìŠµí•œ 'Retrain'ëª¨ë¸ì„ ë§Œë“¤ê³  ê° ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„ ë“±ì„ í‰ê°€í•˜ì„¸ìš” (train forgetset, train remainset, test forgetset, test remainset...)\n",
        "\n",
        "\n",
        "- 1-1 Result\n",
        "  - Train Remainset Accuracy (others): 99.99%\n",
        "  - Train Forgetset Accuracy (deer): 100.00%\n",
        "  - Test Remainset Accuracy (others): 83.08%\n",
        "  - Test Forgetset Accuracy (deer): 82.50%\n",
        "\n",
        "    <img src='https://drive.google.com/uc?id=1I5Eud6OMcq1G7I6mbq4lIRl3TM_py7lH' width='450' height='450' />\n",
        "\n",
        "- 1-2 Result\n",
        "  - Train Remainset Accuracy (others): 99.99%\n",
        "  - Train Forgetset Accuracy (deer): 0.00%\n",
        "  - Test Remainset Accuracy (others): 83.93%\n",
        "  - Test Forgetset Accuracy (deer): 0.00%\n",
        "\n",
        "    <img src='https://drive.google.com/uc?id=19dLiQm8hgs5sPy3JRgKkIlfSs53xHXQZ' width='450' height='450' />"
      ],
      "metadata": {
        "id": "7TNAqX432DwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "boundary unlearning: rapid forgetting of deep networks via shifting the decision boundary(CVPR 2023)\n",
        "\n",
        "Original ëª¨ë¸(Cifar10 ë°ì´í„°ì„¸íŠ¸ë¡œ í›ˆë ¨ëœ ëª¨ë¸) í•™ìŠµ ë° ì‹œê°í™” ì½”ë“œ\n",
        "Train Remainset Accuracy (others): 99.99%\n",
        "Train Forgetset Accuracy (deer): 100.00%\n",
        "Test Remainset Accuracy (others): 83.08%\n",
        "Test Forgetset Accuracy (deer): 82.50%\n",
        "\n",
        "to do : ëª¨ë“ˆí™”\n",
        "\"\"\"\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "save_dir = \"./Result\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë¶„ë¦¬ í•¨ìˆ˜\n",
        "def split_datasets(dataset, target_class):\n",
        "    forget_indices = [i for i, (_, label) in enumerate(dataset) if label == target_class]\n",
        "    remain_indices = [i for i, (_, label) in enumerate(dataset) if label != target_class]\n",
        "    return Subset(dataset, forget_indices), Subset(dataset, remain_indices)\n",
        "\n",
        "# ì‚­ì œ ëŒ€ìƒ í´ë˜ìŠ¤ - deer(4)\n",
        "target_class = 4\n",
        "\n",
        "train_forgetset, train_remainset = split_datasets(train_dataset, target_class)\n",
        "test_forgetset, test_remainset = split_datasets(test_dataset, target_class)\n",
        "\n",
        "train_forget_loader = DataLoader(train_forgetset, batch_size=64, shuffle=True)\n",
        "train_remain_loader = DataLoader(train_remainset, batch_size=64, shuffle=True)\n",
        "test_forget_loader = DataLoader(test_forgetset, batch_size=64, shuffle=False)\n",
        "test_remain_loader = DataLoader(test_remainset, batch_size=64, shuffle=False)\n",
        "\n",
        "# ResNet18 ëª¨ë¸ ì´ˆê¸°í™”\n",
        "def create_model(num_classes=10):\n",
        "    model = resnet18(pretrained=True)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# í•™ìŠµ í•¨ìˆ˜\n",
        "def train_model(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# í‰ê°€ í•¨ìˆ˜\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# Original ëª¨ë¸ í•™ìŠµ\n",
        "best_accuracy = 0.0\n",
        "epoch = 50\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "original_model = create_model(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(original_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# í•™ìŠµ\n",
        "for i in range(epoch):\n",
        "    train_model(original_model, train_loader, criterion, optimizer, device)\n",
        "    accuracy = evaluate_model(original_model, test_loader, device)\n",
        "    print(f\"Epoch {i+1}, Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # ìµœê³  ì •í™•ë„ ê°±ì‹  ì‹œ\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "\n",
        "        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥\n",
        "        best_weight_path = os.path.join(save_dir, \"Org_Model.pth\")\n",
        "        torch.save(original_model.state_dict(), best_weight_path)\n",
        "        print(f\"New best accuracy: {accuracy * 100:.2f}% - Weights saved at {best_weight_path}\")\n",
        "\n",
        "\n",
        "print(\"Training complete. Best weights saved as 'best_model.pth'\")\n",
        "\n",
        "\n",
        "# TSNE ì‹œê°í™” í•¨ìˆ˜\n",
        "def plot_tsne(model, remain_loader, forget_loader, device, title, classes, save_path):\n",
        "    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜\n",
        "    features = []  # ì¶”ì¶œëœ íŠ¹ì§• ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
        "    predictions = []  # ì˜ˆì¸¡ í´ë˜ìŠ¤ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
        "    dataset_type = []  # \"Remain\" ë˜ëŠ” \"Forget\" êµ¬ë¶„ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "    # Remainset ë°ì´í„°ì—ì„œ íŠ¹ì§• ë° ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in remain_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            features.append(outputs.cpu().numpy())  # ëª¨ë¸ ì¶œë ¥ íŠ¹ì§• ì €ì¥\n",
        "            predictions.append(torch.argmax(outputs, dim=1).cpu().numpy())  # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
        "            dataset_type.extend([\"Remain\"] * inputs.size(0))  # Remainset ë°ì´í„° êµ¬ë¶„ ì¶”ê°€\n",
        "\n",
        "    # Forgetset ë°ì´í„°ì—ì„œ íŠ¹ì§• ë° ì˜ˆì¸¡ ê²°ê³¼ ì¶”ì¶œ\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in forget_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            features.append(outputs.cpu().numpy())  # ëª¨ë¸ ì¶œë ¥ íŠ¹ì§• ì €ì¥\n",
        "            predictions.append(torch.argmax(outputs, dim=1).cpu().numpy())  # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
        "            dataset_type.extend([\"Forget\"] * inputs.size(0))  # Forgetset ë°ì´í„° êµ¬ë¶„ ì¶”ê°€\n",
        "\n",
        "    # ë°ì´í„°ë¥¼ í•©ì¹¨\n",
        "    features = np.concatenate(features, axis=0)\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    dataset_type = np.array(dataset_type)\n",
        "\n",
        "    # TSNEë¡œ ì°¨ì› ì¶•ì†Œ\n",
        "    # tsne = TSNE(n_components=2, perplexity=40, learning_rate=300, n_iter=3000, random_state=42)\n",
        "    tsne = TSNE(n_components=2, perplexity=50, learning_rate=500, n_iter=4000, random_state=42)\n",
        "    reduced_features = tsne.fit_transform(features)\n",
        "\n",
        "    # ì‹œê°í™”\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    colors = plt.cm.Paired(np.linspace(0, 1, len(classes)))\n",
        "\n",
        "    # Remainset ë°ì´í„° ì‹œê°í™”\n",
        "    remain_features = reduced_features[dataset_type == \"Remain\"]\n",
        "    remain_predictions = predictions[dataset_type == \"Remain\"]\n",
        "    for i, class_name in enumerate(classes):\n",
        "        indices_remain = remain_predictions == i\n",
        "        plt.scatter(\n",
        "            remain_features[indices_remain, 0],\n",
        "            remain_features[indices_remain, 1],\n",
        "            label=f\"Class {i+1} ({class_name})\",\n",
        "            alpha=0.5,\n",
        "            s=30,\n",
        "            color=colors[i]\n",
        "        )\n",
        "\n",
        "    # Forgetset ë°ì´í„° ì‹œê°í™”\n",
        "    forget_features = reduced_features[dataset_type == \"Forget\"]\n",
        "    forget_predictions = predictions[dataset_type == \"Forget\"]\n",
        "    for i, class_name in enumerate(classes):\n",
        "        indices_forget = forget_predictions == i\n",
        "        plt.scatter(\n",
        "            forget_features[indices_forget, 0],\n",
        "            forget_features[indices_forget, 1],\n",
        "            alpha=0.6,\n",
        "            s=30,\n",
        "            edgecolor=\"black\",\n",
        "            color=colors[i]\n",
        "        )\n",
        "\n",
        "    plt.title(title, fontsize=20, fontweight=\"bold\")\n",
        "    plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5), fontsize=\"small\", markerscale=1.0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # ì‹œê°í™” ì´ë¯¸ì§€ ì €ì¥\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"t-SNE visualization saved at {save_path}\")\n",
        "\n",
        "best_weight_path = os.path.join(save_dir, \"Org_Model.pth\")\n",
        "\n",
        "final_model = create_model(num_classes=10).to(device)\n",
        "final_model.load_state_dict(torch.load(best_weight_path))\n",
        "final_model.eval()\n",
        "print(f\"Loaded best weights from {best_weight_path}\")\n",
        "\n",
        "# ê° ë°ì´í„°ì…‹ì—ì„œ ì •í™•ë„ í‰ê°€\n",
        "train_forget_acc = evaluate_model(final_model, train_forget_loader, device)\n",
        "train_remain_acc = evaluate_model(final_model, train_remain_loader, device)\n",
        "test_forget_acc = evaluate_model(final_model, test_forget_loader, device)\n",
        "test_remain_acc = evaluate_model(final_model, test_remain_loader, device)\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(f\"Train Remainset Accuracy (others): {train_remain_acc * 100:.2f}%\")\n",
        "print(f\"Train Forgetset Accuracy (deer): {train_forget_acc * 100:.2f}%\")\n",
        "print(f\"Test Remainset Accuracy (others): {test_remain_acc * 100:.2f}%\")\n",
        "print(f\"Test Forgetset Accuracy (deer): {test_forget_acc * 100:.2f}%\")\n",
        "\n",
        "# TSNE ì‹œê°í™”\n",
        "tsne_predictions_path = os.path.join(save_dir, \"tsne_Org_Model.png\")\n",
        "print(\"\\nGenerating TSNE visualization...\")\n",
        "\n",
        "plot_tsne(\n",
        "    final_model,\n",
        "    test_remain_loader,\n",
        "    test_forget_loader,\n",
        "    device,\n",
        "    \"TSNE Visualization(Original Model)\",\n",
        "    classes,\n",
        "    tsne_predictions_path\n",
        ")"
      ],
      "metadata": {
        "id": "mI36vZS0EOgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "boundary unlearning: rapid forgetting of deep networks via shifting the decision boundary(CVPR 2023)\n",
        "\n",
        "Retrain ëª¨ë¸(Dr, ì¦‰ forgetting dataë¥¼ ì œì™¸í•œ ë°ì´í„°ì„¸íŠ¸ë¡œ í•™ìŠµí•œ ëª¨ë¸) í•™ìŠµ ë° ì‹œê°í™” ì½”ë“œ\n",
        "Train Remainset Accuracy (others): 99.99%\n",
        "Train Forgetset Accuracy (deer): 0.00%\n",
        "Test Remainset Accuracy (others): 83.93%\n",
        "Test Forgetset Accuracy (deer): 0.00%\n",
        "\n",
        "to do : ëª¨ë“ˆí™”\n",
        "\"\"\"\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "save_dir = \"./Result\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë¶„ë¦¬ í•¨ìˆ˜\n",
        "def split_datasets(dataset, target_class):\n",
        "    forget_indices = [i for i, (_, label) in enumerate(dataset) if label == target_class]\n",
        "    remain_indices = [i for i, (_, label) in enumerate(dataset) if label != target_class]\n",
        "    return Subset(dataset, forget_indices), Subset(dataset, remain_indices)\n",
        "\n",
        "# ì‚­ì œ ëŒ€ìƒ í´ë˜ìŠ¤ - deer(4)\n",
        "target_class = 4\n",
        "\n",
        "train_forgetset, train_remainset = split_datasets(train_dataset, target_class)\n",
        "test_forgetset, test_remainset = split_datasets(test_dataset, target_class)\n",
        "\n",
        "train_forget_loader = DataLoader(train_forgetset, batch_size=64, shuffle=True)\n",
        "train_remain_loader = DataLoader(train_remainset, batch_size=64, shuffle=True)\n",
        "test_forget_loader = DataLoader(test_forgetset, batch_size=64, shuffle=False)\n",
        "test_remain_loader = DataLoader(test_remainset, batch_size=64, shuffle=False)\n",
        "\n",
        "# ResNet18 ëª¨ë¸\n",
        "def create_model(num_classes=10):\n",
        "    model = resnet18(pretrained=True)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# í•™ìŠµ í•¨ìˆ˜\n",
        "def train_model(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# í‰ê°€ í•¨ìˆ˜\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "best_accuracy = 0.0\n",
        "epoch = 50\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "retrain_model = create_model(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(retrain_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# í•™ìŠµ\n",
        "for i in range(epoch):\n",
        "    train_model(retrain_model, train_remain_loader, criterion, optimizer, device)\n",
        "    accuracy = evaluate_model(retrain_model, test_remain_loader, device)\n",
        "    print(f\"Epoch {i+1}, Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # ìµœê³  ì„±ëŠ¥ ì €ì¥\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "\n",
        "        best_weight_path = os.path.join(save_dir, \"retrain_model.pth\")\n",
        "        torch.save(retrain_model.state_dict(), best_weight_path)\n",
        "        print(f\"New best accuracy: {accuracy * 100:.2f}% - Weights saved at {best_weight_path}\")\n",
        "\n",
        "final_model = create_model(num_classes=10).to(device)\n",
        "final_model.load_state_dict(torch.load(best_weight_path))\n",
        "final_model.eval()\n",
        "print(f\"Loaded best weights from {best_weight_path}\")\n",
        "\n",
        "train_forget_acc = evaluate_model(final_model, train_forget_loader, device)\n",
        "train_remain_acc = evaluate_model(final_model, train_remain_loader, device)\n",
        "test_forget_acc = evaluate_model(final_model, test_forget_loader, device)\n",
        "test_remain_acc = evaluate_model(final_model, test_remain_loader, device)\n",
        "\n",
        "print(f\"Train Remainset Accuracy (others): {train_remain_acc * 100:.2f}%\")\n",
        "print(f\"Train Forgetset Accuracy (deer): {train_forget_acc * 100:.2f}%\")\n",
        "print(f\"Test Remainset Accuracy (others): {test_remain_acc * 100:.2f}%\")\n",
        "print(f\"Test Forgetset Accuracy (deer): {test_forget_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "5A95009BrnF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Baseline ë§Œë“¤ê¸°\n",
        "2-1. ì•ì„œ í•™ìŠµí•œ Original ëª¨ë¸ì„ ê°€ì§€ê³  ë…¼ë¬¸ baselineìœ¼ë¡œ ì„¤ëª…ëœ 'Random Labels' ê¸°ë²•ì— ëŒ€í•´ Machine Unlearningì„ ìˆ˜í–‰í•˜ê³  ì •í™•ë„ ë“±ì„ í‰ê°€í•˜ì„¸ìš”. (Random Labels ê¸°ë²•ì€ forgetting dataì— ì„ì˜ì˜ ë¼ë²¨ì„ ë¶€ì—¬í•˜ì—¬ fine-tuneí•©ë‹ˆë‹¤.)\n",
        "\n",
        "[2-1 Result]\n",
        "- Train Remainset Accuracy (others): 89.39%\n",
        "- Train Forgetset Accuracy (deer): 8.88%\n",
        "- Test Remainset Accuracy (others): 73.58%\n",
        "- Test Forgetset Accuracy (deer): 6.70%"
      ],
      "metadata": {
        "id": "bKcISx0EKRY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selective forgetting of deep networks at a finer level than samples(CoRR 2020)\n",
        "\n",
        "[ëœë¤ ë¼ë²¨ ë””ìŠ¤í‹¸ë ˆì´ì…˜(Random Label Distillation, RLD)]\n",
        "íŠ¹ì • ì…ë ¥ ë°ì´í„°ê°€ ë„¤íŠ¸ì›Œí¬ì— ë¬´ì‘ìœ„ ë¼ë²¨ì„ í•™ìŠµí•˜ë„ë¡ ê°•ì œí•˜ì—¬ ê¸°ì¡´ í•™ìŠµëœ ì •ë³´ë¥¼ \"ë§ê°\" ì‹œí‚´\n",
        "\n",
        "[í•™ìŠµ ëª©í‘œ]\n",
        "Df(ìŠì–´ì•¼í•  ë°ì´í„°)ì— ëŒ€í•œ ì„±ëŠ¥ì€ ë–¨ì–´ëœ¨ë¦¬ê³ , Dr(ë‚˜ë¨¸ì§€ ë°ì´í„°)ì˜ ì„±ëŠ¥ì€ ìœ ì§€í•˜ëŠ” ê²ƒì´ ëª©í‘œ\n",
        "ì´ë¥¼ ìœ„í•´ 2ê°€ì§€ì˜ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê²°í•©í•˜ì—¬ ì‚¬ìš©\n",
        "\n",
        "### ë§ê° ì†ì‹¤(Lf)\n",
        "- Df ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì„ ë‚®ì¶”ê¸° ìœ„í•´ ì‚¬ìš©\n",
        "- ëœë¤ ë¼ë²¨ ë””ìŠ¤í‹¸ë ˆì´ì…˜(Random Label Distillation, RLD)ì„ ì‚¬ìš©í•˜ì—¬ Dfë¥¼ ë§ê° ì‹œí‚´\n",
        "- $ L_{\\text{RLD}}(f_\\theta, x_f) = L_{\\text{cls}}(f_\\theta(x_f), u) $\n",
        "  - $f_\\theta(x_f)$ : ëª¨ë¸ì´ xfì— ëŒ€í•´ ì¶œë ¥í•œ ë¡œì§“ ê°’\n",
        "  - u : xfì— ëŒ€í•´ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ í´ë˜ìŠ¤\n",
        "  - $L_{\\text{cls}}$ : Softmax Cross-Entropy Loss\n",
        "  - ì¦‰, xfë¥¼ ì›ë˜ì˜ ì •ë‹µ ë¼ë²¨ì´ ì•„ë‹Œ ëœë¤ ë¼ë²¨ë¡œ í•™ìŠµì‹œí‚´ìœ¼ë¡œì¨, í•´ë‹¹ ë°ì´í„°ì— ëŒ€í•œ ê¸°ì¡´ í•™ìŠµ ë‚´ìš©ì„ ìŠê²Œ í•œë‹¤.\n",
        "\n",
        "### ê¸°ì–µ ì†ì‹¤(R)\n",
        "- Dr ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©\n",
        "- Elastic Weight Consolidation(EWC) ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ê¸°ì¡´ ë°ì´í„°ì˜ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ìœ ì§€í•˜ë„ë¡ ì •ê·œí™”\n",
        "- $ R(f_\\theta, f_{\\theta_{\\text{old}}}) = (\\theta - \\theta_{\\text{old}})^T F(\\theta_{\\text{old}})(\\theta - \\theta_{\\text{old}}) $\n",
        "  - $\\theta$ : í˜„ì¬ í•™ìŠµ ì¤‘ì¸ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ë²¡í„°\n",
        "  - $\\theta_{\\text{old}}$ : ê³¼ê±° ëª¨ë¸(original ëª¨ë¸)ì˜ íŒŒë¼ë¯¸í„° ë²¡í„°\n",
        "  - $F(\\theta_{\\text{old}})$ : Fisher ì •ë³´ í–‰ë ¬. ê° íŒŒë¼ë¯¸í„°ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ëƒ„\n",
        "-  Fisher ì •ë³´ í–‰ë ¬($F(\\theta)_{ii}$)\n",
        "  - íŒŒë¼ë¯¸í„° ğœƒì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ ë¯¼ê°ë„(ê° íŒŒë¼ë¯¸í„°ê°€ ì†ì‹¤ì— ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ì— ëŒ€í•œ ì •ë³´)\n",
        "  - $F(\\theta_{\\text{old}})_{ii} = |D|^{-1} \\sum_{(x, l) \\in D} \\left[ \\frac{\\partial L_{\\text{cls}}(f_\\theta(x), l)}{\\partial \\theta_i} \\right]^2$\n",
        "  - ëª¨ë“  íŒŒë¼ë¯¸í„° $\\theta_i$ì— ëŒ€í•´ ì†ì‹¤í•¨ìˆ˜ $L_{\\text{cls}}$ì˜ í¸ë¯¸ë¶„ì˜ ì œê³±ì˜ í‰ê· ì˜ ëŒ€ê° ì„±ë¶„\n",
        "- $ \\frac{\\partial R}{\\partial \\theta_i} = 2 \\cdot F(\\theta_{\\text{old}})_{ii} \\cdot (\\theta_i - \\theta_{\\text{old}, i})$\n",
        "  - EWC ì†ì‹¤ í•­ì— ëŒ€í•´ $\\theta_i$ì— ëŒ€í•œ ê¸°ìš¸ê¸°\n",
        "  - ì¦‰, Fisher ì •ë³´ê°€ í´ìˆ˜ë¡ ê¸°ìš¸ê¸°ì˜ í¬ê¸°ê°€ ì»¤ì§„ë‹¤.\n",
        "  - ê²½ì‚¬í•˜ê°•ë²•ì—ì„œ í° ê¸°ìš¸ê¸°ëŠ” íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ë¥¼ ê°•í•˜ê²Œ ìœ ë„í•˜ì—¬ ì†ì‹¤ì„ ë¹ ë¥´ê²Œ ì¤„ì¸ë‹¤.\n",
        "  - ë”°ë¼ì„œ ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°(Fisher ì •ë³´ê°€ í°)ëŠ” ì›ë˜ ê°’ì— ë¨¸ë¬´ë¥´ë ¤ëŠ” ê²½í–¥ì´ ê°•í•´ì§€ê³ , í° ë³€í™”ê°€ ì œí•œëœë‹¤.\n",
        "- <í™•ì¸í•´ë³´ê¸°> ì´ì°¨í˜•ì‹(Quadratic Forms)...????\n",
        "\n",
        "### ìµœì¢… ì†ì‹¤ í•¨ìˆ˜\n",
        "- $L = L_f(f_\\theta, D_f) + \\lambda_{\\text{KL}} \\cdot R(f_\\theta, f_{\\theta_{\\text{old}}})$\n",
        "  - $\\lambda_{\\text{KL}}$ : Rì— ëŒ€í•œ ê°€ì¤‘ì¹˜(0 ì´ìƒ, ê³ ì •ê°’)\n",
        "\n"
      ],
      "metadata": {
        "id": "q5f-NQVhaL9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "boundary unlearning: rapid forgetting of deep networks via shifting the decision boundary(CVPR 2023)\n",
        "\n",
        "Random Labels ëª¨ë¸(ëœë¤ìœ¼ë¡œ ë¼ë²¨ë§í•œ forgetting dataë¥¼ ì‚¬ìš©í•´ original ëª¨ë¸ì„ íŒŒì¸íŠœë‹) í•™ìŠµ ë° ì‹œê°í™” ì½”ë“œ\n",
        "\n",
        "Train Remainset Accuracy (others): 89.39%\n",
        "Train Forgetset Accuracy (deer): 8.88%\n",
        "Test Remainset Accuracy (others): 73.58%\n",
        "Test Forgetset Accuracy (deer): 6.70%\n",
        "\n",
        "to do :\n",
        "- ëª¨ë“ˆí™”\n",
        "- ASR ì¶”ê°€\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Selective forgetting of deep networks at a finer level than samples(CoRR 2020)\n",
        "\n",
        "[ëœë¤ ë¼ë²¨ ë””ìŠ¤í‹¸ë ˆì´ì…˜(Random Label Distillation, RLD)]\n",
        "íŠ¹ì • ì…ë ¥ ë°ì´í„°ê°€ ë„¤íŠ¸ì›Œí¬ì— ë¬´ì‘ìœ„ ë¼ë²¨ì„ í•™ìŠµí•˜ë„ë¡ ê°•ì œí•˜ì—¬ ê¸°ì¡´ í•™ìŠµëœ ì •ë³´ë¥¼ \"ë§ê°\" ì‹œí‚´\n",
        "\n",
        "[í•™ìŠµ ëª©í‘œ]\n",
        "Df(ìŠì–´ì•¼í•  ë°ì´í„°)ì— ëŒ€í•œ ì„±ëŠ¥ì€ ë–¨ì–´ëœ¨ë¦¬ê³ , Dr(ë‚˜ë¨¸ì§€ ë°ì´í„°)ì˜ ì„±ëŠ¥ì€ ìœ ì§€í•˜ëŠ” ê²ƒì´ ëª©í‘œ\n",
        "ì´ë¥¼ ìœ„í•´ 2ê°€ì§€ì˜ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê²°í•©í•˜ì—¬ ì‚¬ìš©\n",
        "\n",
        "1. ë§ê° ì†ì‹¤(Lf)\n",
        "Df ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì„ ë‚®ì¶”ê¸° ìœ„í•´ ì‚¬ìš©\n",
        "ëœë¤ ë¼ë²¨ ë””ìŠ¤í‹¸ë ˆì´ì…˜(Random Label Distillation, RLD)ì„ ì‚¬ìš©í•˜ì—¬ Dfë¥¼ ë§ê° ì‹œí‚´\n",
        "\n",
        "2. ê¸°ì–µ ì†ì‹¤(R)\n",
        "Dr ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©\n",
        "Elastic Weight Consolidation(EWC) ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ê¸°ì¡´ ë°ì´í„°ì˜ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ìœ ì§€í•˜ë„ë¡ ì •ê·œí™”\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# ëœë¤ ì‹œë“œ ê³ ì •\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "save_dir = \"./Result\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë¶„ë¦¬ í•¨ìˆ˜\n",
        "def split_datasets(dataset, target_class):\n",
        "    forget_indices = [i for i, (_, label) in enumerate(dataset) if label == target_class]\n",
        "    remain_indices = [i for i, (_, label) in enumerate(dataset) if label != target_class]\n",
        "    return Subset(dataset, forget_indices), Subset(dataset, remain_indices)\n",
        "\n",
        "target_class = 4  # deer\n",
        "\n",
        "train_forgetset, train_remainset = split_datasets(train_dataset, target_class)\n",
        "test_forgetset, test_remainset = split_datasets(test_dataset, target_class)\n",
        "\n",
        "train_forget_loader = DataLoader(train_forgetset, batch_size=64, shuffle=True)\n",
        "train_remain_loader = DataLoader(train_remainset, batch_size=64, shuffle=True)\n",
        "test_forget_loader = DataLoader(test_forgetset, batch_size=64, shuffle=False)\n",
        "test_remain_loader = DataLoader(test_remainset, batch_size=64, shuffle=False)\n",
        "\n",
        "def create_model(num_classes=10):\n",
        "    model = resnet18(pretrained=True)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "# ëœë¤ ë¼ë²¨ ìƒì„± í•¨ìˆ˜\n",
        "def generate_random_labels(labels, num_classes):\n",
        "    random_labels = torch.randint(0, num_classes, labels.size(), device=labels.device)\n",
        "    while (random_labels == labels).any():\n",
        "        random_labels = torch.randint(0, num_classes, labels.size(), device=labels.device)\n",
        "    return random_labels\n",
        "\n",
        "# Random Label Distillation(RLD) Loss : CrossEntropyLoss ì‚¬ìš©\n",
        "# ë§ê° ì†ì‹¤(Lf) ì°¸ê³ \n",
        "def random_label_distillation_loss(outputs, random_labels):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    return criterion(outputs, random_labels)\n",
        "\n",
        "# Elastic Weight Consolidation (EWC) Loss\n",
        "# ê¸°ì–µ ì†ì‹¤(R) ì°¸ê³ \n",
        "def ewc_loss(model, fisher_matrix, old_weights, lambda_ewc):\n",
        "    ewc_loss_value = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if name in fisher_matrix:\n",
        "            fisher = fisher_matrix[name]   # fisher ì •ë³´(íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„)\n",
        "            old_weight = old_weights[name] # original ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ê°’\n",
        "            # (í˜„ì¬ íŒŒë¼ë¯¸í„° - ê³¼ê±° íŒŒë¼ë¯¸í„°)^2ì— Fisher ì •ë³´ë¡œ ê°€ì¤‘ì¹˜ ë¶€ì—¬ í›„ í•©ì‚°\n",
        "            ewc_loss_value += (fisher * (param - old_weight).pow(2)).sum()\n",
        "    # EWC ì†ì‹¤ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
        "    return lambda_ewc * ewc_loss_value\n",
        "\n",
        "# Fisher ì •ë³´ ê³„ì‚° í•¨ìˆ˜\n",
        "# original ëª¨ë¸ë¡œë¶€í„° íŒŒë¼ë¯¸í„° ì¤‘ìš”ë„ ê³„ì‚°\n",
        "def calculate_fisher_information(model, dataloader, device):\n",
        "    # fisherí–‰ë ¬ ì´ˆê¸°í™”(ëª¨ë¸ì˜ ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ 0ìœ¼ë¡œ ì´ˆê¸°í™”)\n",
        "    fisher_matrix = {name: torch.zeros_like(param) for name, param in model.named_parameters() if param.requires_grad}\n",
        "    model.eval()\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        model.zero_grad()\n",
        "        # fÎ¸(x)\n",
        "        outputs = model(inputs)\n",
        "        # Lcls(fÎ¸(x),l)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        # âˆ‚Lcls(fÎ¸(x),l) / âˆ‚Î¸i\n",
        "        loss.backward()\n",
        "\n",
        "        # ìµœì¢… fisher ì •ë³´ í–‰ë ¬\n",
        "        # ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸° ì œê³±ì˜ í‰ê· ìœ¼ë¡œ ì •ì˜\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad and param.grad is not None:\n",
        "                fisher_matrix[name] += param.grad.pow(2) / len(dataloader)\n",
        "\n",
        "    return fisher_matrix\n",
        "\n",
        "# í•™ìŠµ í•¨ìˆ˜\n",
        "def train_model(model, dataloader, optimizer, fisher_matrix, old_weights, lambda_ewc, device):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # ëœë¤ ë¼ë²¨ ìƒì„±\n",
        "        random_labels = generate_random_labels(labels, len(classes))\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # ë§ê° ì†ì‹¤ (RLD)\n",
        "        rld_loss = random_label_distillation_loss(outputs, random_labels)\n",
        "\n",
        "        # ê¸°ì–µ ì†ì‹¤ (EWC)\n",
        "        ewc_loss_value = ewc_loss(model, fisher_matrix, old_weights, lambda_ewc)\n",
        "\n",
        "        # ì´ ì†ì‹¤\n",
        "        loss = rld_loss + ewc_loss_value\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# í‰ê°€ í•¨ìˆ˜\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "Random_Labels_Model = create_model(num_classes=len(classes)).to(device)\n",
        "original_model = create_model(num_classes=len(classes)).to(device)\n",
        "\n",
        "original_weights_path = \"./Result/Org_Model.pth\"\n",
        "original_model.load_state_dict(torch.load(original_weights_path))\n",
        "print(f\"Loaded original model weights from {original_weights_path}\")\n",
        "\n",
        "Random_Labels_Model.load_state_dict(copy.deepcopy(original_model.state_dict()))\n",
        "\n",
        "# original_model ê°€ì¤‘ì¹˜\n",
        "old_weights = {name: param.clone().detach() for name, param in original_model.named_parameters()}\n",
        "# ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ Fisher ì •ë³´ í–‰ë ¬ ê³„ì‚°\n",
        "fisher_matrix = calculate_fisher_information(original_model, train_loader, device)\n",
        "\n",
        "# We set Î»KL = 10^5, lr = 10^âˆ’5 throughout experiments.\n",
        "optimizer = optim.SGD(Random_Labels_Model.parameters(), lr=1e-5)\n",
        "lambda_ewc = 1e5\n",
        "best_tradeoff_score = -float('inf')\n",
        "\n",
        "for epoch in range(10):\n",
        "    print(f\"Epoch {epoch + 1}/10\")\n",
        "    train_model(Random_Labels_Model, train_forget_loader, optimizer, fisher_matrix, old_weights, lambda_ewc, device)\n",
        "\n",
        "    #train_remain_acc = evaluate_model(Random_Labels_Model, train_remain_loader, device)\n",
        "    test_remain_acc = evaluate_model(Random_Labels_Model, test_remain_loader, device)\n",
        "    #train_forget_acc = evaluate_model(Random_Labels_Model, train_forget_loader, device)\n",
        "    test_forget_acc = evaluate_model(Random_Labels_Model, test_forget_loader, device)\n",
        "\n",
        "    # print(f\"Train Remainset Accuracy: {train_remain_acc * 100:.2f}%\")\n",
        "    # print(f\"Train Forgetset Accuracy: {train_forget_acc * 100:.2f}%\")\n",
        "    # print(f\"Test Remainset Accuracy: {test_remain_acc * 100:.2f}%\")\n",
        "    # print(f\"Test Forgetset Accuracy: {test_forget_acc * 100:.2f}%\")\n",
        "\n",
        "    tradeoff_score = test_remain_acc - test_forget_acc\n",
        "\n",
        "    if tradeoff_score > best_tradeoff_score:\n",
        "        best_tradeoff_score = tradeoff_score\n",
        "        best_weight_path = os.path.join(save_dir, \"Random_Labels_Model.pth\")\n",
        "        torch.save(Random_Labels_Model.state_dict(), best_weight_path)\n",
        "        print(f\"New best trade-off score: {tradeoff_score:.4f} - Model saved to {best_weight_path}\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "final_model = create_model(num_classes=10).to(device)\n",
        "final_model.load_state_dict(torch.load(best_weight_path))\n",
        "final_model.eval()\n",
        "print(f\"Loaded best weights from {best_weight_path}\")\n",
        "\n",
        "train_forget_acc = evaluate_model(final_model, train_forget_loader, device)\n",
        "train_remain_acc = evaluate_model(final_model, train_remain_loader, device)\n",
        "test_forget_acc = evaluate_model(final_model, test_forget_loader, device)\n",
        "test_remain_acc = evaluate_model(final_model, test_remain_loader, device)\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(f\"Train Remainset Accuracy (others): {train_remain_acc * 100:.2f}%\")\n",
        "print(f\"Train Forgetset Accuracy (deer): {train_forget_acc * 100:.2f}%\")\n",
        "print(f\"Test Remainset Accuracy (others): {test_remain_acc * 100:.2f}%\")\n",
        "print(f\"Test Forgetset Accuracy (deer): {test_forget_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "aWY-jLegEPy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ë…¼ë¬¸ì˜ ì•„ì´ë””ì–´ êµ¬í˜„í•˜ê¸°\n",
        "3-1. ë…¼ë¬¸ì˜ ë©”ì¸ ì•„ì´ë””ì–´ ì¤‘ í•˜ë‚˜ì¸ 'Boundary Shrink'ë¥¼ êµ¬í˜„í•˜ê³  ê° ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„ ë“±ì„ í‰ê°€í•˜ì„¸ìš”. ì´ë•Œ adversarial attack ê¸°ë²•ì¸ PGD attackë¥¼ ì§ì ‘ êµ¬í˜„í•´ë³´ì„¸ìš” (ì–´ë ¤ìš°ì‹œë©´ ë‹¤ë¥¸ë°ì„œ ì°¾ì•„ì˜¤ì…”ë„ ë©ë‹ˆë‹¤.)\n",
        "\n",
        "3-2. Retrain, Random Labels, Boundary Shrink ì„¸ ê¸°ë²•ì˜ ê²°ê³¼ë¥¼ ì„œë¡œ ë¹„êµí•˜ê³  í‰ê°€í•˜ì„¸ìš”.\n",
        "\n",
        "[3-1 Result]\n",
        "- Train Remainset Accuracy (others): 94.02%\n",
        "- Train Forgetset Accuracy (deer): 3.22%\n",
        "- Test Remainset Accuracy (others): 77.32%\n",
        "- Test Forgetset Accuracy (deer): 2.50%\n",
        "\n",
        "    <img src='https://drive.google.com/uc?id=1NL9QlqVnLxRq0cpxJz81Z6dLjfkf8b6y' width='450' height='450' />\n",
        "\n",
        "[3-2 Result]\n",
        "- Retrain ëª¨ë¸\n",
        "  - Forgetsetì˜ Accuracyê°€ 0%ë¡œ, ì‚­ì œëœ ë°ì´í„°ì— ëŒ€í•œ ì •ë³´ê°€ ì™„ì „íˆ ì œê±°ë¨. í•˜ì§€ë§Œ ì²˜ìŒë¶€í„° ë‹¤ì‹œ í•™ìŠµí•´ì•¼ í•˜ë¯€ë¡œ, ì—°ì‚° ë¹„ìš©ê³¼ ì‹œê°„ì´ ë§¤ìš° í¼.\n",
        "- Random Labels\n",
        "  - Boundary Shrinkë³´ë‹¤ ì ì€ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ Forgetsetì˜ Accuracyë¥¼ ë‚®ì¶œ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, Boundary Shrinkì— ë¹„í•´ ì„±ëŠ¥ì´ ë‚®ë‹¤.\n",
        "- Boundary Shrink\n",
        "  - Forgetset Accuracyê°€ 2.50%ë¡œ, Random Labelsë³´ë‹¤ ê°•ë ¥í•œ Privacy Guaranteeë¥¼ ì œê³µí•œë‹¤.\n",
        "  - ë˜í•œ  Remaining Classì˜ ì„±ëŠ¥ ì†ìƒ(Train Remainset Accuracy 94.02%)ì„ ìµœì†Œí™” í•˜ì˜€ë‹¤.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "  | **Metric**                          | **Original** | **Retrain** | **Random Labels** | **Boundary Shrink** |\n",
        "|-------------------------------------|--------------|-------------|--------------------|----------------------|\n",
        "| **Train Remainset Accuracy (others)** | 99.99%       | 99.99%      | 89.39%            | 94.02%              |\n",
        "| **Train Forgetset Accuracy (deer)**  | 100.00%      | 0.00%       | 8.88%             | 3.22%               |\n",
        "| **Test Remainset Accuracy (others)** | 83.08%       | 83.93%      | 73.58%            | 77.32%              |\n",
        "| **Test Forgetset Accuracy (deer)**   | 82.50%       | 0.00%       | 6.70%             | 2.50%               |\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Q31HKVlS-XTPBtHbs6orWKHlry7xSh6h' width='1000' height='450' />\n"
      ],
      "metadata": {
        "id": "szXxDebxKRo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "boundary unlearning: rapid forgetting of deep networks via shifting the decision boundary(CVPR 2023)\n",
        "\n",
        "Boundary Shrink ëª¨ë¸ í•™ìŠµ ë° ì‹œê°í™”(https://www.dropbox.com/scl/fi/j3hgtrvp1vjptk9qz5ck6/Boundary-Unlearning-Code.zip?rlkey=h32gro8ysi4umtolzmi54gbdo&e=1&dl=0)\n",
        "\n",
        "Train Remainset Accuracy (others): 94.02%\n",
        "Train Forgetset Accuracy (deer): 3.22%\n",
        "Test Remainset Accuracy (others): 77.32%\n",
        "Test Forgetset Accuracy (deer): 2.50%\n",
        "\n",
        "to do :\n",
        "- ëª¨ë“ˆí™”\n",
        "- ASR ì¶”ê°€\n",
        "\"\"\"\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.distributions as distributions\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "def split_datasets(dataset, target_class):\n",
        "    forget_indices = [i for i, (_, label) in enumerate(dataset) if label == target_class]\n",
        "    remain_indices = [i for i, (_, label) in enumerate(dataset) if label != target_class]\n",
        "    return Subset(dataset, forget_indices), Subset(dataset, remain_indices)\n",
        "\n",
        "target_class = 4\n",
        "train_forgetset, train_remainset = split_datasets(train_dataset, target_class)\n",
        "test_forgetset, test_remainset = split_datasets(test_dataset, target_class)\n",
        "\n",
        "train_forget_loader = DataLoader(train_forgetset, batch_size=64, shuffle=True)\n",
        "train_remain_loader = DataLoader(train_remainset, batch_size=64, shuffle=True)\n",
        "test_forget_loader = DataLoader(test_forgetset, batch_size=64, shuffle=False)\n",
        "test_remain_loader = DataLoader(test_remainset, batch_size=64, shuffle=False)\n",
        "\n",
        "def create_model(num_classes=10):\n",
        "    model = resnet18(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n",
        "\n",
        "\"\"\"\n",
        "adversarial attackì´ë€? : perturbation(ì‚¬ëŒì˜ ëˆˆìœ¼ë¡œëŠ” êµ¬ë³„ ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ, ëª¨ë¸ì˜ ì˜ˆì¸¡ì— ì˜í–¥ì„ ì£¼ë„ë¡ í•˜ëŠ” ë…¸ì´ì¦ˆ)ë¥¼ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì˜ ì˜¤ë¶„ë¥˜ë¥¼ ìœ ë„í•˜ëŠ” ê³µê²© ê¸°ë²•\n",
        "ëŒ€í‘œì ì¸ ê¸°ë²•ìœ¼ë¡œëŠ” FGSMê³¼ PGDê°€ ì¡´ì¬í•œë‹¤.\n",
        "\n",
        "1. FGSM(Fast Gradient Sign Method)\n",
        "    Explaining and Harnessing Adversarial Examples(ICLR 2015)\n",
        "    ì˜¤ë¶„ë¥˜ë¥¼ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” ìµœì ì˜ perturbation(ë…¸ì´ì¦ˆ, ë¸íƒ€)ë¥¼ ì°¾ëŠ” ë°©ë²•\n",
        "    - ì†ì‹¤í•¨ìˆ˜ Lì˜ ì…ë ¥ xì— ëŒ€í•œ gradientë¥¼ ê³„ì‚°.\n",
        "    - signì„ ì·¨í•˜ì—¬ ë¶€í˜¸(ë°©í–¥)ë§Œì„ ì¶”ì¶œ\n",
        "    - ì¼ì •í•œ ê°’(ì—¡ì‹¤ë¡ )ì„ ì¶”ê°€\n",
        "    í•œë²ˆì˜ ê¸°ìš¸ê¸° ê³„ì‚°ë§Œ ìˆ˜í–‰í•˜ë¯€ë¡œ ë¹ ë¥´ê³  ê³„ì‚° ë¹„ìš©ì´ ë‚®ë‹¤.\n",
        "\n",
        "2. PGD(Projected Gradient Descent)\n",
        "    Towards Deep Learning Models Resistant to Adversarial Attacks(ICLR 2018)\n",
        "    FGSMì„ ë°˜ë³µ ìˆ˜í–‰í•˜ì—¬ gradientë¥¼ ì—…ë°ì´íŠ¸\n",
        "    ê° ê³¼ì •ì—ì„œ  step size(=lr) ì§€ì •\n",
        "    -> ë” ê°•ë ¥í•œ adversarial sampleì„ ìƒì„± ê°€ëŠ¥í•˜ë‹¤.\n",
        "    ê° ê³¼ì •ì—ì„œ projectionì„ í†µí•´ perturbation(ë¸íƒ€)ì„ ì¼ì • ë²”ìœ„(L-infinity norm)ë‚´ë¡œ ì œí•œí•œë‹¤.\n",
        "\"\"\"\n",
        "class LinfPGD:\n",
        "    def __init__(self, model=None, bound=None, step=None, iters=None, norm=False, random_start=False, device=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model : ê³µê²© ëŒ€ìƒ ëª¨ë¸ - test ëª¨ë¸\n",
        "            bound (float): í—ˆìš© ë²”ìœ„ (L-infinity ê¸°ì¤€ epsilon ê°’) - 0.1\n",
        "            step (float): í•œ ë²ˆì˜ ì—…ë°ì´íŠ¸ì—ì„œ ë³€ê²½í•  í¬ê¸° (step size) - 2 / 255\n",
        "            iters (int): ì ëŒ€ì  ìƒ˜í”Œ ìƒì„±ì„ ë°˜ë³µí•  íšŸìˆ˜ - 5\n",
        "            norm (bool): ì…ë ¥ ë°ì´í„°ë¥¼ ì •ê·œí™”í• ì§€ ì—¬ë¶€ - True\n",
        "            random_start (bool): ì´ˆê¸° ìƒ˜í”Œì— ë¬´ì‘ìœ„ ë³€ë™ì„ ì¶”ê°€í• ì§€ ì—¬ë¶€ - True\n",
        "            device\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.bound = bound\n",
        "        self.step = step\n",
        "        self.iter = iters\n",
        "        self.norm = norm\n",
        "        if self.norm:\n",
        "            # CIFAR10 ë°ì´í„°ì…‹ ê¸°ì¤€ ì •ê·œí™” í‰ê·  ë° í‘œì¤€í¸ì°¨\n",
        "            self.mean = (0.4914, 0.4822, 0.2265)\n",
        "            self.std = (0.2023, 0.1994, 0.2010)\n",
        "        self.rand = random_start\n",
        "        self.device = device\n",
        "        # CrossEntropyLoss ì‚¬ìš©\n",
        "        self.criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    def perturb(self, x, y, model=None, bound=None, step=None, iters=None, x_nat=None, device=None):\n",
        "        \"\"\"\n",
        "        PGD(Projected Gradient Descent) attackì„ ìˆ˜í–‰í•˜ì—¬ ì ëŒ€ì  ìƒ˜í”Œ ìƒì„±\n",
        "\n",
        "        Args:\n",
        "            x : ì›ë³¸ ì…ë ¥ ë°ì´í„°\n",
        "            y : ì›ë³¸ ì…ë ¥ ë°ì´í„°ì˜ ì •ë‹µ ë¼ë²¨\n",
        "            model : ê³µê²© ëŒ€ìƒ ëª¨ë¸ - test ëª¨ë¸\n",
        "            bound (float): í—ˆìš© ë²”ìœ„ (L-infinity ê¸°ì¤€ epsilon ê°’) - 0.1\n",
        "            step (float): í•œ ë²ˆì˜ ì—…ë°ì´íŠ¸ì—ì„œ ë³€ê²½í•  í¬ê¸° (step size) - 2 / 255\n",
        "            iters (int): ì ëŒ€ì  ìƒ˜í”Œ ìƒì„±ì„ ë°˜ë³µí•  íšŸìˆ˜ - 5\n",
        "            x_nat (torch.Tensor): ì •ê·œí™”ë˜ì§€ ì•Šì€ ì›ë³¸ ì…ë ¥ ë°ì´í„° - None\n",
        "            device\n",
        "        \"\"\"\n",
        "        criterion = self.criterion   # CrossEntropyLoss\n",
        "        model = model or self.model\n",
        "        bound = bound or self.bound\n",
        "        step = step or self.step\n",
        "        iters = iters or self.iter\n",
        "        device = device or self.device\n",
        "\n",
        "        # ëª¨ë¸ ê·¸ë ˆë””ì–¸íŠ¸ ì´ˆê¸°í™”\n",
        "        model.zero_grad()\n",
        "\n",
        "        # ì •ê·œí™”ë˜ì§€ ì•Šì€ ì›ë³¸ ë°ì´í„°ë¥¼ ìƒì„± (ê¸°ë³¸ ì„¤ì •).\n",
        "        # torch.detach().clone() : ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ê·¸ë˜í”„ì—ì„œ ë¶„ë¦¬ í›„ ë³µì‚¬í•˜ì—¬ ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ê³µê°„ì— ì €ì¥\n",
        "        if x_nat is None:\n",
        "            x_nat = self.inverse_normalize(x.detach().clone().to(device))\n",
        "        else:\n",
        "            x_nat = self.inverse_normalize(x_nat.detach().clone().to(device))\n",
        "\n",
        "        # ì ëŒ€ì  ìƒ˜í”Œ ì´ˆê¸°í™”\n",
        "        x_adv = x.detach().clone().requires_grad_(True).to(device)\n",
        "\n",
        "        # ì ëŒ€ì  ìƒ˜í”Œì— ë…¸ì´ì¦ˆ ì¶”ê°€í•œ ìƒíƒœë¡œ ì‹œì‘\n",
        "        if self.rand:\n",
        "            # í—ˆìš© ë²”ìœ„ ë‚´ì—ì„œ ëœë¤í•œ ë…¸ì´ì¦ˆë¥¼ ìƒì„±\n",
        "            rand_perturb_dist = distributions.uniform.Uniform(-bound, bound)\n",
        "            rand_perturb = rand_perturb_dist.sample(sample_shape=x_adv.shape).to(device)\n",
        "            # ìƒì„±í•œ ë…¸ì´ì¦ˆ ì¶”ê°€ ë° ë²”ìœ„ ì œí•œ\n",
        "            x_adv = self.clamper(self.inverse_normalize(x_adv) + rand_perturb, self.inverse_normalize(x_nat),\n",
        "                                 bound=bound, inverse_normalized=True)\n",
        "            # ë…¸ì´ì¦ˆ ì¶”ê°€í•œ ì´ˆê¸° x_adv ì •ê·œí™” ë° ì´ì‚°í™”\n",
        "            x_adv = self.normalize(self.discretize(x_adv)).detach().clone().requires_grad_(True)\n",
        "\n",
        "        # PGD attack ë°˜ë³µ -> ìµœì ì˜ ì ëŒ€ì  ìƒ˜í”Œ ë°˜í™˜\n",
        "        for i in range(iters):\n",
        "            # í˜„ì¬ ì ëŒ€ì  ìƒ˜í”Œì— ëŒ€í•œ ëª¨ë¸(original ëª¨ë¸) ì¶œë ¥\n",
        "            adv_pred = model(x_adv)\n",
        "            # loss ê³„ì‚°. ëª¨ë¸ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ì •ë‹µ ê°„ì˜ ì†ì‹¤ ê³„ì‚°\n",
        "            loss = criterion(adv_pred, y)\n",
        "            # ì†ì‹¤í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° - ìˆ˜ì‹(2)\n",
        "            loss.backward()\n",
        "            # ê·¸ë˜ë””ì–¸íŠ¸ì˜ ë¶€í˜¸ ê³„ì‚° - ìˆ˜ì‹(2)\n",
        "            grad_sign = x_adv.grad.data.detach().sign()\n",
        "            # x_advë¥¼ ì—…ë°ì´íŠ¸: x_advì— step sizeë§Œí¼ perturbation ì¶”ê°€ - ìˆ˜ì‹(2)\n",
        "            x_adv = self.inverse_normalize(x_adv) + grad_sign * step\n",
        "            # ë²”ìœ„ ì œí•œ\n",
        "            x_adv = self.clamper(x_adv, x_nat, bound=bound, inverse_normalized=True)\n",
        "            model.zero_grad()\n",
        "\n",
        "        # ìµœì¢…ì ìœ¼ë¡œ ì ëŒ€ì  ìƒ˜í”Œ ë°˜í™˜(ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ x)\n",
        "        return x_adv.detach().to(device)\n",
        "\n",
        "    # ì…ë ¥ ë°ì´í„° ì •ê·œí™”\n",
        "    def normalize(self, x):\n",
        "        # x shape : (batch_size, channel, height, width)\n",
        "        # normì´ Trueì¼ ê²½ìš°ì—ëŠ” ì •ê·œí™” í›„ ë°˜í™˜\n",
        "        # normì´ Falseì¼ ê²½ìš°ì—ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
        "        if self.norm:\n",
        "            y = x.clone().to(x.device)\n",
        "            # ê° ì²´ë„ë³„ë¡œ ì •ê·œí™” (x - í‰ê· )/í‘œì¤€í¸ì°¨\n",
        "            y[:, 0, :, :] = (y[:, 0, :, :] - self.mean[0]) / self.std[0]  # Rì²´ë„\n",
        "            y[:, 1, :, :] = (y[:, 1, :, :] - self.mean[1]) / self.std[1]  # Gì²´ë„\n",
        "            y[:, 2, :, :] = (y[:, 2, :, :] - self.mean[2]) / self.std[2]  # Bì²´ë„\n",
        "            return y\n",
        "        return x\n",
        "\n",
        "    # ì…ë ¥ ë°ì´í„° ì—­ì •ê·œí™”\n",
        "    def inverse_normalize(self, x):\n",
        "        if self.norm:\n",
        "            y = x.clone().to(x.device)\n",
        "            y[:, 0, :, :] = y[:, 0, :, :] * self.std[0] + self.mean[0]\n",
        "            y[:, 1, :, :] = y[:, 1, :, :] * self.std[1] + self.mean[1]\n",
        "            y[:, 2, :, :] = y[:, 2, :, :] * self.std[2] + self.mean[2]\n",
        "            return y\n",
        "        return x\n",
        "\n",
        "    # ë°ì´í„° ì´ì‚°í™” -> ê³„ì‚° ì´ë“?\n",
        "    def discretize(self, x):\n",
        "        return torch.round(x * 255) / 255\n",
        "\n",
        "    # ì ëŒ€ì  ìƒ˜í”Œ(x_adv)ì´ ì›ë³¸ ìƒ˜í”Œ(x_nat)ì˜ í—ˆìš© ë²”ìœ„(bound)ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì œí•œ(clamp)í•˜ëŠ” í•¨ìˆ˜\n",
        "    # torch.clamp(x, a, b) : xì˜ ê°’ì„ a, b ë²”ìœ„ ì•ˆìœ¼ë¡œ ì¡°ì •í•´ì£¼ëŠ” í•¨ìˆ˜\n",
        "    def clamper(self, x_adv, x_nat, bound=None, inverse_normalized=False):\n",
        "        # ì—­ì •ê·œí™” ì•ˆë˜ì–´ ìˆìœ¼ë©´ ì—­ì •ê·œí™” ì§„í–‰\n",
        "        if not inverse_normalized:\n",
        "            x_adv = self.inverse_normalize(x_adv)\n",
        "            x_nat = self.inverse_normalize(x_nat)\n",
        "\n",
        "        # í”½ì…€ ê°’ ì°¨ì´ë¥¼ boundë¡œ ì œí•œ\n",
        "        clamp_delta = torch.clamp(x_adv - x_nat, -bound, bound)\n",
        "\n",
        "        # ë…¸ì´ì¦ˆ ë”í•´ì£¼ê¸°\n",
        "        x_adv = x_nat + clamp_delta\n",
        "        # ë²”ìœ„ ì œí•œ(0~1)ì‚¬ì´ë¡œ\n",
        "        x_adv = torch.clamp(x_adv, 0., 1.)\n",
        "        # ì´ì‚°í™”(discretize) ë° ì •ê·œí™”(normalize) í›„ ë°˜í™˜, ì´í›„ ì¶”ê°€ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ê°€ëŠ¥í•˜ë„ë¡ requires_grad ì„¤ì •\n",
        "        return self.normalize(self.discretize(x_adv)).clone().detach().requires_grad_(True)\n",
        "\n",
        "\n",
        "def inf_generator(iterable):\n",
        "    iterator = iterable.__iter__()\n",
        "    while True:\n",
        "        try:\n",
        "            yield iterator.__next__()\n",
        "        except StopIteration:\n",
        "            iterator = iterable.__iter__()\n",
        "\n",
        "###################################################################################################################################################\n",
        "\n",
        "# boundary_shrink\n",
        "def boundary_shrink(ori_model, train_forget_loader, dt, dv, test_loader, device, evaluate,\n",
        "                    bound=0.1, step=2 / 255, iter=5, poison_epoch=10, forget_class=0, path='./'):\n",
        "\n",
        "    # ì •ê·œí™”, ë¬´ì‘ìœ„ ì‹œì‘\n",
        "    norm = True\n",
        "    random_start = True\n",
        "\n",
        "    # test_model : ì ëŒ€ì  ìƒ˜í”Œ ìƒì„±ì— ì‚¬ìš©\n",
        "    # unlearn_model : í•™ìŠµ(unlearning)ì— ì‚¬ìš©\n",
        "    test_model = copy.deepcopy(ori_model).to(device)\n",
        "    unlearn_model = copy.deepcopy(ori_model).to(device)\n",
        "\n",
        "    # LinfPGD ì¸ìŠ¤í„´ìŠ¤ ìƒì„±(ì ëŒ€ì  ìƒ˜í”Œ ìƒì„±ì„ ìœ„í•œ)\n",
        "    adv = LinfPGD(test_model, bound, step, iter, norm, random_start, device)\n",
        "\n",
        "    # loss / ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
        "    # ë…¼ë¬¸ : For the fine-tune process in Boundary Unlearning we use a learning rate of 10âˆ’5 for 10 epochs.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(unlearn_model.parameters(), lr=1e-5, momentum=0.9)\n",
        "\n",
        "    # num_hits = 0\n",
        "    # num_sum = 0\n",
        "    best_tradeoff_score = -float('inf')\n",
        "    best_weight_path = os.path.join(path, \"shrink_model.pth\")\n",
        "\n",
        "    # boundary shrink í•™ìŠµ ê³¼ì •\n",
        "    for epoch in range(poison_epoch):\n",
        "        print(f\"Epoch {epoch + 1}/{poison_epoch}\")\n",
        "\n",
        "        for x, y in tqdm(train_forget_loader, desc=f\"Epoch {epoch + 1} Batches\"):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # testëª¨ë¸(ì ëŒ€ì  ìƒ˜í”Œ ìƒì„±ì„ ìœ„í•œ ëª¨ë¸)ì„ ì‚¬ìš©í•´ ì ëŒ€ì  ìƒ˜í”Œì„ ìƒì„±\n",
        "            # ì ëŒ€ì  ìƒ˜í”Œ : ë…¸ì´ì¦ˆ ì¶”ê°€ëœ x\n",
        "            test_model.eval()\n",
        "            x_adv = adv.perturb(x, y, model=test_model, device=device)\n",
        "\n",
        "            # ì ëŒ€ì  ìƒ˜í”Œì— ëŒ€í•œ ì˜ˆì¸¡ê°’ ì¶œë ¥ ë° ì˜ˆì¸¡ ë¼ë²¨ ê³„ì‚°\n",
        "            adv_logits = test_model(x_adv)\n",
        "            pred_label = torch.argmax(adv_logits, dim=1)\n",
        "\n",
        "            # num_hits += (y != pred_label).float().sum()\n",
        "            # num_sum += y.shape[0]\n",
        "\n",
        "            # unlearnëª¨ë¸ í•™ìŠµ\n",
        "            unlearn_model.train()\n",
        "            optimizer.zero_grad()\n",
        "            # unlearnëª¨ë¸ì— ì ëŒ€ì  ìƒ˜í”Œì„ ë„£ì–´ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì¶œë ¥\n",
        "            ori_logits = unlearn_model(x)\n",
        "            # loss ê³„ì‚° - ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì ëŒ€ì  ìƒ˜í”Œì˜ ë¼ë²¨(ë°”ë€ ë¼ë²¨) ì†ì‹¤ ê³„ì‚°\n",
        "            loss = criterion(ori_logits, pred_label)\n",
        "            # ì†ì‹¤í•¨ìˆ˜ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë° ì—…ë°ì´íŠ¸\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        test_remain_acc = evaluate_model(unlearn_model, test_remain_loader, device)\n",
        "        test_forget_acc = evaluate_model(unlearn_model, test_forget_loader, device)\n",
        "\n",
        "        print(f\"Test Remainset Accuracy: {test_remain_acc * 100:.2f}%\")\n",
        "        print(f\"Test Forgetset Accuracy: {test_forget_acc * 100:.2f}%\")\n",
        "        tradeoff_score = test_remain_acc - test_forget_acc\n",
        "\n",
        "        if tradeoff_score > best_tradeoff_score:\n",
        "            best_tradeoff_score = tradeoff_score\n",
        "            torch.save(unlearn_model.state_dict(), best_weight_path)\n",
        "            print(f\"New best trade-off score: {tradeoff_score:.4f} - Model saved to {best_weight_path}\")\n",
        "\n",
        "    # asr = (num_hits / num_sum).float()\n",
        "    # print('Attack Success Ratio (ASR):', asr)\n",
        "\n",
        "    return unlearn_model\n",
        "\n",
        "###################################################################################################################################################\n",
        "\n",
        "\n",
        "# ì •í™•ë„ í‰ê°€ í•¨ìˆ˜\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# ëª¨ë¸ ìƒì„± ë° ë¡œë“œ\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ori_model = create_model(num_classes=10).to(device)\n",
        "original_weights_path = \"./Result/Org_Model.pth\"\n",
        "ori_model.load_state_dict(torch.load(original_weights_path))\n",
        "print(f\"Original model loaded from {original_weights_path}\")\n",
        "\n",
        "save_dir = \"./Result\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "evaluation = ''\n",
        "\n",
        "unlearn_model = boundary_shrink(ori_model, train_forget_loader, train_dataset, test_dataset,\n",
        "                                                    test_loader, device, evaluation,\n",
        "                                                    forget_class=target_class, path=save_dir)\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤ ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ\n",
        "best_weight_path = os.path.join(save_dir, \"shrink_model.pth\")\n",
        "final_model = create_model(num_classes=10).to(device)\n",
        "final_model.load_state_dict(torch.load(best_weight_path))\n",
        "\n",
        "# ê° ë°ì´í„°ì…‹ì—ì„œ ì •í™•ë„ í‰ê°€\n",
        "train_forget_acc = evaluate_model(final_model, train_forget_loader, device)\n",
        "train_remain_acc = evaluate_model(final_model, train_remain_loader, device)\n",
        "test_forget_acc = evaluate_model(final_model, test_forget_loader, device)\n",
        "test_remain_acc = evaluate_model(final_model, test_remain_loader, device)\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(f\"Train Remainset Accuracy (others): {train_remain_acc * 100:.2f}%\")\n",
        "print(f\"Train Forgetset Accuracy (deer): {train_forget_acc * 100:.2f}%\")\n",
        "print(f\"Test Remainset Accuracy (others): {test_remain_acc * 100:.2f}%\")\n",
        "print(f\"Test Forgetset Accuracy (deer): {test_forget_acc * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "iVo6tLkBEQYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. í‰ê°€\n",
        "4-1. ë…¼ë¬¸ê³¼ ì‹¤í—˜ì˜ ì£¼ìš” contribution, weakness, ê°œì„ í•  ì , êµ¬í˜„ ì‹œ ëŠë‚€ì  ë“±, ë³¸ì¸ì˜ ìƒê°ì„ ììœ ë¡­ê²Œ ì ì–´ì£¼ì„¸ìš”\n",
        "\n",
        "1. ì£¼ìš” Contribution\n",
        "  - ê²°ì • ê²½ê³„ë¥¼ ì´ë™ì‹œì¼œ íŠ¹ì • í´ë˜ìŠ¤ ì •ë³´ë¥¼ ì œê±°í•˜ëŠ” ìµœì´ˆì˜ unlearning ê¸°ë²•ì„ ì œì•ˆí•¨\n",
        "  - Utilityì™€ Privacy ë³´ì¥ì„ ë™ì‹œì— ë§Œì¡±í•˜ë©´ì„œë„ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ë§ê° ì„±ëŠ¥ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦í•¨\n",
        "\n",
        "2. Weakness\n",
        "  - Boundary ShrinkëŠ” ì ëŒ€ì  ìƒ˜í”Œ ìƒì„± ê³¼ì •ì—ì„œ ë†’ì€ computational costë¥¼ ìš”êµ¬í•˜ë©°, ëŒ€ê·œëª¨ ë°ì´í„°ì„¸íŠ¸ë‚˜ ë³µì¡í•œ ëª¨ë¸ì—ì„œëŠ” ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„±ì´ ìˆë‹¤ê³  ìƒê°í•œë‹¤.\n",
        "\n",
        "3. ê°œì„ í•  ì \n",
        "  - ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ê³¼ ë³µì¡í•œ ëª¨ë¸ì— ëŒ€í•´ì„œë„ ì¼ê´€ëœ ì„±ëŠ¥ì„ ìœ ì§€í•˜ëŠ”ì§€ ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•˜ë‹¤ê³  ìƒê°í•œë‹¤.\n",
        "\n",
        "4. ëŠë‚€ì \n",
        "  - ë…¼ë¬¸ì—ì„œëŠ” Remaining Class(Dr)ì˜ Accuracyê°€ 99%, 98% ìˆ˜ì¤€ìœ¼ë¡œ ìœ ì§€ëœë‹¤ê³  ë˜ì–´ ìˆìœ¼ë‚˜, ì‹¤í—˜ ê²°ê³¼ëŠ” 90%, 94%ë¡œ ë‹¤ì†Œ ë‚®ê²Œ ë‚˜ì™”ë‹¤. ì´ëŠ” ì‹¤í—˜ í™˜ê²½ì´ë‚˜ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­, í˜¹ì€ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œì˜ ì°¨ì´ ë•Œë¬¸ì¼ ê°€ëŠ¥ì„±ì´ ìˆì–´ ì¶”ê°€ì ì¸ í™•ì¸ì´ í•„ìš”í•´ ë³´ì¸ë‹¤.\n",
        "  - ë…¼ë¬¸ì—ì„œ ì œì•ˆëœ ìˆ˜ì‹(Random Labelsì˜ ë§ê° ì†ì‹¤ í•¨ìˆ˜ì™€ Boundary Shrinkì˜ PGD ê³µê²©)ì„ ì´í•´í•˜ê³  êµ¬í˜„í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ëŠê¼ˆë‹¤. íŠ¹íˆ, ì œì•ˆëœ ë°©ë²•ë“¤ì´ ê°œë…ì ìœ¼ë¡œ ì´í•´ëŠ” ë˜ì—ˆìœ¼ë‚˜, ì´ë¥¼ ì‹¤ì œ ì½”ë“œë¡œ êµ¬í˜„í•˜ëŠ” ë° ì‹œê°„ì´ ê±¸ë ¸ë‹¤.\n",
        "  - ê²°ë¡ ì ìœ¼ë¡œ, ë…¼ë¬¸ì„ ê¹Šì´ ì´í•´í•˜ê³  êµ¬í˜„í•˜ëŠ” ê³¼ì •ì—ì„œ ìˆ˜í•™ì  ì§ê´€ê³¼ ì½”ë”© ì—­ëŸ‰ì„ ê°•í™”í•´ì•¼ í•œë‹¤ëŠ” í•„ìš”ì„±ì„ ëŠê¼ˆìœ¼ë©°, ì´ë¥¼ ìœ„í•´ ì²´ê³„ì ì¸ í•™ìŠµê³¼ ì‹¤ìŠµì´ í•„ìš”í•˜ë‹¤ê³  ëŠê¼ˆë‹¤."
      ],
      "metadata": {
        "id": "eCcbem5vKR4p"
      }
    }
  ]
}